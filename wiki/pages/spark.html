<html>
	<head>
		<title>Wiki</title>
		 <link rel=stylesheet href=../wiki.css>
		 <link rel=icon href=icon.jpg>
	</head>
	<body>
		<h1>Python Spark</h1>
		<h3>Frequently needed imports</h3>
		<p>
			<code>from pyspark.sql import functions as f</code></br>
			<code>from pyspark.sql.types import *</code></br>
		</p>
		<h3>Get number of partitions on Dataframe</h3>
		<p>
			<code>df.rdd.getNumPartitions()</code></br>
		</p>
		<h3>Change number of partitions of Dataframe</h3>
		<p>
			<code>df = df.repartition(30)</code></br>
		</p>
		<h3>Read from .csv</h3>
		<p><code>df = spark.read.options(header=True, inferSchema=True).csv("dataframe.csv")</code></p>
		<h3>Write to .csv</h3>
		<p><code>df.repartition(1).write.csv('/mnt/dataframe.csv', header = True)</code></p>

		<h3>Dataframe exploration</h3>
		<p>	
			<code>print(df.count(), len(df.columns))</code></br>
			<code>print(df.columns, df.dtypes)</code></br>
			<code>df.printSchema()</code></br>
			<code>df.show(20)</code></br>
			<code>df.limit(20)</code></br>
			<code>df.sample(False, 0.1)</code>
		</p>
		<h3>Unique column values with count</h3>
		<p>	
			<code>column="dpid"</code></br>
			<code>df.groupBy(column).count()</code></br>
		</p>

		<h3>Dataframe columns</h3>
		<p>	
			<code>df.select('column_name')</code></br>
			<code>df.select('struct_name.*')</code></br>
			<code>df.select(df.sid, f.explode(df.dpi).alias("dpi")).select(df.sid, "dpi.*")</code>
		</p>
		<h3>Dataframe filtering</h3>
		<p>	
			<code>df.where(f.col('dpi.dpt').isNull())</code></br>
			<code>df.where(f.col('dpi.dpt').isNotNull())</code></br>
			<code>df.where((f.col('dpi.dpt') == 'DSP') & (f.col("ATC.curated") == False))</code>
		</p>
		<h3>Replacing columns based on condition</h3>
		<p>	
			<code>df.withColumn('ccod', f.when(f.col('ccod') == '', 'Empty').otherwise(f.col('ccod')))</code></br>
		</p>
		<h3>Filling null values</h3>
		<p>	
			<code>df.fillna(0)</code></br>
			<code>df.fillna(0, subset=['a', 'b'])</code></br>
		</p>
		<h3>Dataframe modification</h3>
		<p>	
			<code>df.withColumnRenamed('old', 'new')</code></br>
			<code>df.withColumn('value', 1 / df.age)</code></br>
			<code>df.withColumn('logdisp', f.log(df.disp))</code></br>
			<code>df.withColumn("decision", f.when(f.col("dpid") == 1001708, True).otherwise(False))</code></br>
			<code>df.withColumn('cond', f.when(df.mpg > 20, 1).when(df.cyl == 6, 2).otherwise(3))</code></br>
			<code>df.drop('strenght')</code></br>
			<code>df.dropDuplicates(['age', 'name'])</code></br>
		</p>
		<h3>Dataframe aggregations</h3>
		<p>	
			<code>df.groupBy("gaid").agg(f.count("sid").alias("num_sessions"))</code></br>
			<code>df.groupBy('A', 'B').pivot('C').sum('D')</code></br>
		</p>
		<h3>Dataframe joins</h3>
		<p>	
			<code>df1.join(df2, on='key')</code></br>
			<code>df1.join(df2, df1.age == df2.age)</code></br>
		</p>

		<h3>Create arbitrary dataframe</h3>
		<p>	
			<code>schema = StructType(</code></br>
			<code>    [StructField("age", IntegerType()),</code></br>
			<code>    StructField("rt", LongType()),</code></br>
			<code>    StructField("ccod", StringType()),</code></br>
			<code>    StructField("label", IntegerType())])</code></br></br>
			<code>data = [[20, 1549974188869, 'US', 1],</code></br>
			<code>    [None, None, 'US', 1],</code></br>
			<code>    [80, 1549972905314, 'US', 0]]</code></br>
			<code>df = spark.createDataFrame(data, schema=schema)</code></br>
		</p>
		<h3>Dataframe iteration</h3>
		<p>	
			<code>for row in df.rdd.collect():</code></br>
			<code>    print(row)</code></br>
			<code>    print(row['column'])</code></br>
		</p>
		<h1>Scala Spark</h1>
		<h3>Frequently needed imports</h3>
		<p>
			<code>import org.apache.spark.sql.functions._</code></br>
			<code>import org.apache.spark.sql.types.{StructField, StructType, IntegerType, DoubleType, StringType, BooleanType}</code>
		</p>
		<h3>String interpolation</h3>
		<p>
			<code>val animal = "fox"</code></br>
			<code>val item = "fence"</code></br>
			<code>val text = s"Little brown ${animal} jumped over red ${item}"</code>
		</p>
		<h3>Get row at specific index</h3>
		<p>
			<code>val index = 5</code></br>
			<code>val row = df.rdd.take(index).last</code>
		</p>
		<h3>Convert Dataframe to Pandas</h3>
		<p>
			<code>%scala</code></br>
			<code>df.createOrReplaceTempView("df")</code></br></br>
			<code>%py</code></br>
			<code>df = sqlContext.table("df").toPandas()</code></br>
			<code>df.head()</code>
		</p>
		<h3>Create arbitrary dataframe</h3>
		<p>
		<code>// Class needs to be defined in separate cell</code></br>
		<code>case class MyClass(dpid:Integer, asId:Integer, ccod:String, decision: Integer)</code></br></br>
		<code>val schema = StructType( Seq (</code></br>
		<code>    StructField( "dpid", IntegerType, true),</code></br>
		<code>    StructField( "asId", IntegerType, true),</code></br>
		<code>    StructField( "ccod", StringType, true),</code></br>
		<code>    StructField( "decision", IntegerType, true)</code></br>
		<code>  ))</code></br>
		<code>val dataframeValues: List[MyClass] = List(</code></br>
		<code>    MyClass(1, 5, "US", 1),</code></br>
		<code>    MyClass(2, 6, "DE", 1),</code></br>
		<code>    MyClass(3, 7, "GB", 0)</code></br>
		<code>)</code></br>
		<code>val rdd: RDD[MyClass] =  sc.parallelize( dataframeValues )</code></br>
		<code>val rowRDD: RDD[Row] = rdd.map( (i: MyClass) â‡’ Row(i.dpid, i.asId, i.ccod, i.decision))</code></br>
		<code>val df = sqlContext.createDataFrame( rowRDD, schema )</code></br>
		</p>
	</body>
</html>
